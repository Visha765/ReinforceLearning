\documentclass[a4paper,11pt]{jsarticle}

\usepackage{mypackage}
% 数式
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
% 画像
\usepackage[dvipdfmx]{graphicx}
\usepackage{float}

\usepackage[margin=20truemm]{geometry}

\usepackage{listings, jlisting}

\lstset{
   basicstyle=\ttfamily\scriptsize,
   commentstyle=\textit,
   classoffset=1,
   keywordstyle=\bfseries,
   frame=tRBl,
   framesep=5pt,
   showstringspaces=false,
   numbers=left,
   firstnumber = 1,
   stepnumber=1,
   numberstyle=\tiny,
   tabsize=2,
   breaklines = true
}

\begin{document}

\title{2022年度 主専攻実験 強化学習\\
  中間レポート}
\author{学類:情報科学類 学籍番号:202012006 氏名:門馬圭佑}
\date{\today}
\maketitle

\section{実験目的}



\section{実装}
予めソースコードの階層と概要を以下に示す。
\begin{verbatim}
├── exec
│   ├── mainQTable.py  ...テーブルQ学習エージェントの学習、評価、可視化を行うmain関数
│   └── mainReplayQTable.py ...上記で経験再生バッファーを利用したmain関数
├── lib
│   ├── model
│   │   ├── agent.py ...学習エージェントのインターフェース
│   │   ├── qTable.py ...Qテーブルクラス
│   │   ├── qTableAgent.py ...テーブルQエージェントクラス
│   │   ├── replayBuffer.py ...経験再生バッファークラス
│   │   └── replayQTableAgent.py ...経験再生バッファーを利用したテーブルQエージェントクラス
│   ├── training
│   │   ├── Evaluation.py ...評価用の関数
│   │   └── Train.py ...学習用の関数
│   └── util
│       ├── boxPlot.py ...箱ヒゲ図を可視化する関数
│       ├── fetchPikle.py ...保存したエージェントのファイルをソートして返す関数
│       └── linePlot.py ...４分位数を折れ線グラフで可視化する関数
└── out
\end{verbatim}

\subsection{方法1:テーブルQ学習の実装}
テーブルQ学習は、状態空間と行動空間を有限個の互いに素な部分集合に分割することで、
最適行動価値関数の推定値をテーブル形式で保持、学習する手法である。\par
今回実験で用いた環境Pendulum-v0では、
状態空間$\mathcal{S}$は角度$\theta [-\pi, \pi]$、角速度$\omega [-8, 8]$、
行動空間$\mathcal{A}$はトルク$\tau [-2, 2]$から成るので、
% $\theta,\omega$は$K$個、$\tau$は$L$個の
状態空間と行動空間の各次元を
数直線上で均等な幅の互いに素な$K,L$個の部分集合に分割し、
状態空間については2次元であるから各次元の部分集合で内積を取ることで、
$\mathcal{S} = \bigcup_{i=1}^{K^2}  \mathcal{S}_{i}$、
% $\mathcal{S}_\omega = \bigcup_{i=1}^{K^2}  \mathcal{S}_\omega _i$、
$\mathcal{A} = \bigcup_{j=1}^L  \mathcal{A}_j$
と分割した。
これにより、行動価値関数の推定値をサイズ$K^2\times L$のテーブル$Q_{[i,j]}$で近似した。
この際、状態$s$、行動$a$から対応するインデックスへの写像を各々$\mathtt{idx}_s(s),\mathtt{idx}_a(a)$として、
$Q(s,a) = Q_{[\mathtt{idx}_s(s), \mathtt{idx}_a(a)]}$のようにテーブルの参照を行なった。
また、行動空間の各部分集合の代表値$a_j \in \mathcal{A_j} (j=1,\cdots,L)$
は集合の上限と下限の平均値、則ち中心点とした。
尚、初期値は$10^{-8}\times\mathcal{N}(0,1)$の分布に従うとした。
そして、学習率を$\alpha$として、各学習ステップで以下の式に基づいて学習を行なった。
\begin{align}
  \delta \longleftarrow r_t + \gamma \max_{1\leq j\leq L} Q_{[\mathtt{idx}_s(s_{t+1}), j]} - Q_{[\mathtt{idx}_s(s_{t}), \mathtt{idx}_a(a_{t})]} \label{Q1}\\
  Q_{[\mathtt{idx}_s(s_{t}), \mathtt{idx}_a(a_{t})]} \longleftarrow Q_{[\mathtt{idx}_s(s_{t}), \mathtt{idx}_a(a_{t})]} + \alpha \delta \label{Q2}
\end{align}

テーブルQ学習では、方策$\pi$は状態$s$の元で行動価値関数を最大にする行動
\begin{align}
  \pi(s) = a_{\arg\max_{1\leq j\leq L} Q_{[\mathtt{idx}_s(s), j]}} \label{Qpi}
\end{align}
で求まり、決定的方策である。
しかし、行動方策$\beta$は、$\epsilon$-greedy方策を用い、以下の式に従って
確率$\epsilon$でランダムな行動を行い、それ以外は方策$\pi$に従うようにした。
\begin{align}
  \beta(s) = 
  \left\{\begin{array}{ll}
    \pi(s) & \mathtt{with probability}\ 1-\epsilon\\
    a_j & \mathtt{with probability}\ \frac{\epsilon}{L}\ \mathtt{for}\ j=1,\cdots,L
  \end{array}\right. \label{Qbeta}
\end{align}
\par
以下、上記のアルゴリズムを実装したプログラムのソースコードについて解説する。

\subsubsection{qTable.py} \label{sec:qTable}
\srcref{qTable}に示すqTable.pyでは、QテーブルをクラスQTableとして実装した。
\par
5-23行目に示すコンストラクタでは、状態空間と行動空間の各次元の分割数K、Lを引数に取り、
% 今回実験で用いた環境Pendulum-v0では状態空間は角度$\theta [-\pi, \pi]$、角速度$\omega [-8, 8]$、行動空間はトルク$\tau [-2, 2]$から成るので、
20,21行目で、行動空間の代表点$a_j$からなる配列を変数actionsに代入し、
23行目でQテーブルとして、$K^2\times L$のサイズで初期値が$10^{-8}\times\mathcal{N}(0,1)$の分布に従う二次元配列のテーブルを作成し、変数tableに代入する。\par
26-29行目の関数xy2thetaは、環境から配列[$\cos \theta, \sin \theta, \omega$]で与えられる状態の変数stateを、配列[$\theta, \omega$]に変換する。
31-37行目の関数digitize\_state、39-41行目の関数digitize\_actionは、各々numpyモジュールのdigitize関数を用いて、
状態の変数state、行動の変数actionから一意のインデックスを返し、
写像$\mathtt{idx}_s(s),\mathtt{idx}_a(a)$を実装している。
また、43-46行目の関数get\_maxQ\_actionは、テーブル$Q_{[i,j]}$より、
引数stateの表す状態sの時に行動価値関数の推定値が最も高い行動
$a_{\arg\max_{1\leq j\leq L} Q_{[\mathtt{idx}_s(s), j]}}$
を返す。
\par
48-55行目の関数update\_Qtableは、以上の関数を利用して、
\matref{Q1}、\matref{Q2}に基づいて、テーブルの更新、則ち学習を行う関数である。

\lstinputlisting[caption = qTable.py ,label = qTable]{lib/model/qTable.py}


\subsubsection{qTableAgent} \label{seq:qTableAgent}
\srcref{qTableAgent}に示すqTableAgent.pyでは、テーブルQ学習を行うエージェントをクラスQTableAgentとして実装した。
\par
まず、5-23行目に示すコンストラクタでは、
状態空間と行動空間の各次元の分割数K、Lを引数に取り、Qテーブルとして
\secref{sec:qTable}で実装したQTableクラスのインスタンスqTableを作成している。
\par
13-17行目の関数save\_modelsは
環境env、現在の学習ステップcurrent\_step、シード値seed、保存先ディレクりのパスpathを引数に取り、
pikleモジュールの関数dumpを利用して、学習エージェントの状態をpickle化し、pathが示すディレクトリにファイルとして保存する。
この際、環境名、学習ステップ、シード値に対して一意のファイル名を付ける。
逆に、
19-22行目の関数load\_modelsはpath、ファイル名filenameを引数に取り、
pathが示すディレクトリから該当するファイル名のpickleファイルを非pickle化し返す。
\par
24-25行目の関数select\_actionは、方策$\pi$を求める関数であり、
テーブルqTableのメソッドselect\_actionを利用し、
\matref{Qpi}に基づいて、次のステップの行動を返す。
また、
27-31行目の関数select\_exploratory\_actionは、行動方策$\beta$を求める関数であり、
\matref{Qbeta}に基づいて、同様に次のステップの行動を返す。
\par
そして、33-34行目の関数trainでは、
テーブルqTableのメソッドupdate\_actionを利用し、

\lstinputlisting[caption = qTableAgent.py ,label = qTableAgent]{lib/model/qTableAgent.py}

\subsection{方法2:経験再生バッファーの実装}

\subsection{実行スクリプトの実装}





\section{実験計画}


\section{結果}

\section{考察}




\end{document}